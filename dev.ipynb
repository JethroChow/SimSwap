{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b91c42fc-000a-4b91-a3e1-ea2995fcf680",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import sys\n",
    "import torch\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import torch.nn.functional as F\n",
    "from torchvision import transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ff3fea36-bd3f-4884-b6d0-50a60d34c077",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ec588a7-0bf0-4ba3-8085-54a195040b87",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from options.test_options import TestOptions\n",
    "\n",
    "sys.argv = sys.argv[:1]\n",
    "\n",
    "opt = TestOptions().parse() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0dae487e-e48b-401a-b1ae-1b1b121743fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 手动给 opt 赋值\n",
    "opt.use_mask = True\n",
    "\n",
    "opt.name = 'people'\n",
    "opt.Arc_path = './checkpoints/arcface_model/arcface_checkpoint.tar'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3bf80804-9582-441f-b7af-d903238cf6b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data1/tides/new/miniconda3/envs/faceSwap/lib/python3.8/site-packages/albumentations/__init__.py:13: UserWarning: A new version of Albumentations is available: 1.4.24 (you have 1.4.18). Upgrade using: pip install -U albumentations. To disable automatic update checks, set the environment variable NO_ALBUMENTATIONS_UPDATE to 1.\n",
      "  check_for_updates()\n"
     ]
    }
   ],
   "source": [
    "import insightface\n",
    "from insightface.app import FaceAnalysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0674a08-6855-482a-b70f-a2d484e9851d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "app = FaceAnalysis(name='antelopev2', root='./checkpoints')\n",
    "app.prepare(ctx_id= 0, det_thresh=0.6, det_size=(640,640))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3d39ef9-1c8a-4262-85ac-703b3d2eb79b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "netArc = torch.load(opt.Arc_path, map_location=torch.device(\"cpu\"))\n",
    "netArc = netArc.to(device)\n",
    "netArc.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "acd59bad-6ca2-4f9f-8b75-b7a043ac9cc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tides/SimSwap/models/base_model.py:68: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  network.load_state_dict(torch.load(save_path))\n"
     ]
    }
   ],
   "source": [
    "from models.fs_model import fsModel\n",
    "\n",
    "face_swap_model = fsModel()\n",
    "face_swap_model.initialize(device, netArc, opt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c397c57a-5ada-4208-84a6-96c126862fef",
   "metadata": {},
   "outputs": [],
   "source": [
    "source_image_path = 'demo_file/source_image/Iron_man.jpg'\n",
    "target_video_path = 'demo_file/target_video/jirou_anni.mp4'\n",
    "result_video_path = 'demo_file/result'\n",
    "crop_size = 224"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65026e25-45a3-4f8f-bbd7-4970185ba009",
   "metadata": {},
   "source": [
    "## Source图像预处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8b1e2477-5dcb-4a1d-9c1a-2336c16b52e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "source_image = cv2.imread(source_image_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dac7237f-1c43-4af6-b7e2-993541b6a8b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1073, 769, 3)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "source_image.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b9b1e009-7692-4170-a6be-708589004d52",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data1/tides/new/miniconda3/envs/faceSwap/lib/python3.8/site-packages/insightface/utils/transform.py:68: FutureWarning: `rcond` parameter will change to the default of machine precision times ``max(M, N)`` where M and N are the input matrix dimensions.\n",
      "To use the future default and silence this warning we advise to pass `rcond=None`, to keep using the old, explicitly pass `rcond=-1`.\n",
      "  P = np.linalg.lstsq(X_homo, Y)[0].T # Affine matrix. 3 x 4\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "dict_keys(['bbox', 'kps', 'det_score', 'landmark_3d_68', 'pose', 'landmark_2d_106', 'gender', 'age', 'embedding'])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "det_result = app.get(source_image, crop_size)\n",
    "det_result[0].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "dab8140f-329e-4d75-9714-d7357ca4d44e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from util import face_align_ffhqandnewarc as face_align"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f6e2f407-47c3-429f-8a3b-f2587cccb256",
   "metadata": {},
   "outputs": [],
   "source": [
    "source_M, _ = face_align.estimate_norm(det_result[0][\"kps\"], crop_size) \n",
    "source_crop_align_image = cv2.warpAffine(source_image, source_M, (crop_size, crop_size), borderValue=0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "54d19f34-e48d-4904-a5f8-8afb8ab95cbc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(224, 224, 3)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "source_crop_align_image.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "fdfa4213-9a5f-4bac-85f2-717dfe3a73d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "source_crop_align_image_pil = Image.fromarray(cv2.cvtColor(source_crop_align_image, cv2.COLOR_BGR2RGB)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "d0bf33a5-5399-4919-aa51-d29c45455634",
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer_Arcface = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ])\n",
    "\n",
    "source_crop_align_image_pil = transformer_Arcface(source_crop_align_image_pil)\n",
    "source_crop_align_image_pil = source_crop_align_image_pil.view(-1, source_crop_align_image_pil.shape[0], source_crop_align_image_pil.shape[1], source_crop_align_image_pil.shape[2])\n",
    "source_crop_align_image_pil = source_crop_align_image_pil.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "d799319e-2862-4e2b-89b5-6895072a77fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[-2.1179, -2.1179, -2.1179,  ..., -2.0837, -2.0837, -2.0837],\n",
       "          [-2.1179, -2.1179, -2.1179,  ..., -2.0837, -2.0837, -2.0837],\n",
       "          [-2.1179, -2.1179, -2.1179,  ..., -2.0837, -2.0837, -2.0837],\n",
       "          ...,\n",
       "          [-2.1179, -2.1179, -2.1179,  ..., -2.1179, -2.1179, -2.1179],\n",
       "          [-2.1179, -2.1179, -2.1179,  ..., -2.1179, -2.1179, -2.1179],\n",
       "          [-2.1179, -2.1179, -2.1179,  ..., -2.1179, -2.1179, -2.1179]],\n",
       "\n",
       "         [[-2.0357, -2.0357, -2.0357,  ..., -2.0357, -2.0357, -2.0357],\n",
       "          [-2.0357, -2.0357, -2.0357,  ..., -2.0357, -2.0357, -2.0357],\n",
       "          [-2.0357, -2.0357, -2.0357,  ..., -2.0357, -2.0357, -2.0357],\n",
       "          ...,\n",
       "          [-2.0357, -2.0357, -2.0357,  ..., -2.0357, -2.0357, -2.0357],\n",
       "          [-2.0357, -2.0357, -2.0357,  ..., -2.0357, -2.0357, -2.0357],\n",
       "          [-2.0357, -2.0357, -2.0357,  ..., -2.0357, -2.0357, -2.0357]],\n",
       "\n",
       "         [[-1.8044, -1.8044, -1.8044,  ..., -1.5604, -1.5256, -1.5256],\n",
       "          [-1.8044, -1.8044, -1.8044,  ..., -1.5604, -1.5256, -1.5256],\n",
       "          [-1.8044, -1.8044, -1.8044,  ..., -1.5604, -1.5256, -1.5256],\n",
       "          ...,\n",
       "          [-1.8044, -1.8044, -1.8044,  ..., -1.8044, -1.8044, -1.8044],\n",
       "          [-1.8044, -1.8044, -1.8044,  ..., -1.8044, -1.8044, -1.8044],\n",
       "          [-1.8044, -1.8044, -1.8044,  ..., -1.8044, -1.8044, -1.8044]]]],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "source_crop_align_image_pil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "fc4dccbd-d8d4-416e-bf15-5adc9350830d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create latent id\n",
    "source_downsample = F.interpolate(source_crop_align_image_pil, size=(112,112))\n",
    "latend_id = netArc(source_downsample)\n",
    "latend_id = F.normalize(latend_id, p=2, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "83d2200c-d509-4ace-bfc0-b0899522f0f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 512])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "latend_id.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e236f99-c76d-4fa7-8390-46e4715c0d93",
   "metadata": {},
   "source": [
    "## target视频预处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "1094f4ca-5a5b-4cda-b450-3b9509f5ddda",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_video = cv2.VideoCapture(target_video_path)\n",
    "target_video.set(cv2.CAP_PROP_POS_FRAMES, 150)\n",
    "\n",
    "ret, frame = target_video.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "a0117432-d62c-4b51-9c5d-14df7be80c6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_detect_results = app.get(frame, crop_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "9c9daefa-d6dc-41f4-96bb-bec187d55cb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_M, _ = face_align.estimate_norm(target_detect_results[0][\"kps\"], crop_size) \n",
    "target_crop_align_image = cv2.warpAffine(frame, target_M, (crop_size, crop_size), borderValue=0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "cf4235f8-3b75-4b27-ab17-c5e9e9bbb823",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _totensor(array):\n",
    "    tensor = torch.from_numpy(array)\n",
    "    img = tensor.transpose(0, 1).transpose(0, 2).contiguous()\n",
    "    return img.float().div(255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "343a8a8f-a3ab-45a9-a517-3ca787a0d021",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_crop_align_tenor = _totensor(cv2.cvtColor(target_crop_align_image, cv2.COLOR_BGR2RGB))[None,...].cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "ae79a18e-4992-494c-87d4-9031ecbbe681",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3, 224, 224])"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_crop_align_tenor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "8fb83b11-912b-4199-90da-f269c90a50ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "swap_result = face_swap_model(None, target_crop_align_tenor, latend_id, None, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "af543baf-b4a1-4abe-9826-9f951e438979",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3, 224, 224])"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "swap_result.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "bd97f480-6dac-431b-9a9a-15dbd5818282",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "swap_result = swap_result.squeeze(0)  # 去掉 batch 维度\n",
    "swap_result = swap_result.permute(1, 2, 0) \n",
    "swap_result = swap_result.cpu()\n",
    "swap_result = (swap_result * 255).clamp(0, 255).byte().numpy()\n",
    "cv2.imwrite('swap_result.jpg', swap_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e98a5de3-c21c-419a-90bd-2f5ab7cb41d4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (faceSwap)",
   "language": "python",
   "name": "faceswap"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
